{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1e1ccb-9d0c-42b5-8355-bc4522fba824",
   "metadata": {},
   "source": [
    "**Sections:**<a name=\"contents\"></a>\n",
    "\n",
    "[Control data download](#control_data_download)\n",
    "\n",
    "[Create symbolink link copies for all experimental fastq files](#symb_link_input)\n",
    "\n",
    "[Quality control](#QC_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33182800-a2e4-47c3-8c18-590b03541527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division,\n",
    "                        print_function, unicode_literals)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# general purpose packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats as smstats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap\n",
    "import rpy2\n",
    "\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('seaborn')\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['backend'] = \"Qt5Agg\"\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from adjustText import adjust_text\n",
    "import builtins\n",
    "%matplotlib inline\n",
    "\n",
    "# for normalization\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "# for working with yaml files\n",
    "import ruamel.yaml\n",
    "\n",
    "# for working with genomic intervals\n",
    "import pyranges as pr\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb77454-c2f4-4c14-aa4a-c37d6ab961da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvalue_star(pval, thr=0.05):\n",
    "    if thr == 0.05:\n",
    "        if pval < 0.001:\n",
    "            return \"***\"\n",
    "        elif pval < 0.01:\n",
    "            return \"**\"\n",
    "        elif pval < 0.05:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"ns\"\n",
    "    elif thr == 0.1:\n",
    "        if pval < 0.001:\n",
    "            return \"***\"\n",
    "        elif pval < 0.01:\n",
    "            return \"**\"\n",
    "        elif pval < 0.1:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"ns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c68b8f6-6004-4243-8dc3-5053639c85d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths to subdirectories\n",
    "subdirs = {}\n",
    "\n",
    "subdirs['main_project_dir'] = '/scicore/home/zavolan/GROUP/RBP_perturbational_networks/'\n",
    "subdirs['wf_dir'] = '/scicore/home/zavolan/mirono0000/Projects/RBP_perturbational_networks/WF/'\n",
    "\n",
    "# tandem pas WF dir\n",
    "subdirs['tandem_PAS_dir'] = '/scicore/home/zavolan/mirono0000/libs/tandem-pas/'\n",
    "\n",
    "subdirs['human_annotation_dir'] = '/scicore/home/zavolan/GROUP/Genomes/homo_sapiens/'\n",
    "\n",
    "# shared project folder \n",
    "subdirs['shared_project_dir'] = subdirs['main_project_dir']\n",
    "subdirs['temp_dir'] = subdirs['shared_project_dir']+'temp_dir/'\n",
    "subdirs['slurm_dir'] = subdirs['temp_dir']+'slurm/'\n",
    "subdirs['scripts_dir'] = subdirs['shared_project_dir']+'scripts/'\n",
    "subdirs['figures_dir'] = subdirs['shared_project_dir']+'figures/'\n",
    "subdirs['fastq_dir'] = subdirs['shared_project_dir']+'input_fastq/'\n",
    "subdirs['metadata_dir'] = subdirs['shared_project_dir']+'metadata/'\n",
    "\n",
    "subdirs['wf_runs_dir'] = subdirs['shared_project_dir']+'wf_runs/'\n",
    "\n",
    "# paths to files\n",
    "file_paths = {}\n",
    "### genome annotation files\n",
    "file_paths['human_genome_file'] = subdirs['human_annotation_dir']+'GRCh38.primary_assembly.genome.fa'\n",
    "file_paths['human_annotation_file'] = subdirs['human_annotation_dir']+'hg38_v42/gencode.v42.annotation.gtf'\n",
    "file_paths['human_RNAcentral_annotation_file'] = subdirs['human_annotation_dir']+'hg38_v42/homo_sapiens.GRCh38.gff3.gz'\n",
    "file_paths['human_enriched_annotation_file'] = subdirs['human_annotation_dir']+'hg38_v42/enriched.gencode.v42.annotation.gtf'\n",
    "\n",
    "file_paths['human_polyAsite_atlas'] = subdirs['human_annotation_dir']+'hg38_v42/atlas.clusters.2.0.GRCh38.96.bed.gz'\n",
    "\n",
    "os.system('mkdir -p '+' '.join(list(subdirs.values()))) # create all subdirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4c3dc-5324-4ad9-8eb8-0cbacf05afde",
   "metadata": {},
   "source": [
    "# Make enriched gtf file for mouse and human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd8b50f4-1edc-4957-b471-1b9748bd3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "organisms = ['human']\n",
    "\n",
    "for organism in organisms:\n",
    "    command = 'samtools faidx '+file_paths[organism+'_genome_file']\n",
    "    out = subprocess.check_output(command, shell=True)\n",
    "\n",
    "    gtf_df = pd.read_csv(file_paths[organism+'_annotation_file'],delimiter=\"\\t\",index_col=None,header=None,skiprows=5)\n",
    "\n",
    "    rna_central = pd.read_csv(file_paths[organism+'_RNAcentral_annotation_file'],delimiter=\"\\t\",index_col=None,header=None,skiprows=1,compression='gzip')\n",
    "    rna_central[2] = rna_central[2].str.replace('noncoding_exon','exon')\n",
    "    rna_central['gene_biotype'] = rna_central[8].str.split(';type=|;',expand=True)[1]\n",
    "    rna_central['gene_source'] = 'RNA_central'\n",
    "    rna_central['gene_id'] = rna_central[8].str.split('ID=|;|:',expand=True)[4]\n",
    "\n",
    "    rna_central_exons = rna_central.loc[rna_central[2]=='exon'].copy().reset_index(drop=True)\n",
    "    rna_central_exons['exon_id'] = rna_central_exons['gene_id']+'.transcript'+'.'+rna_central_exons[8].str.split('ID=|;|:',expand=True)[5]\n",
    "    rna_central_exons['exon_number'] = rna_central_exons[8].str.split('ID=|;|:',expand=True)[5].str.split('exon',expand=True).iloc[:, -1]\n",
    "    rna_central_exons[8] = 'gene_id \"'+rna_central_exons['gene_id']+'\"; transcript_id \"'+rna_central_exons['gene_id']+'.transcript'+'\"; exon_number \"'+rna_central_exons['exon_number']+'\"; gene_source \"'+rna_central_exons['gene_source']+'\"; gene_type \"'+rna_central_exons['gene_biotype']+'\"; transcript_source \"'+rna_central_exons['gene_source']+'\"; transcript_type \"'+rna_central_exons['gene_biotype']+'\"; exon_id \"'+rna_central_exons['exon_id']+'\"; tag \"'+rna_central['gene_source']+'\";'\n",
    "    rna_central_exons['order']=3\n",
    "\n",
    "    rna_central_exons['transcript_id'] = rna_central_exons['gene_id']+'.transcript'\n",
    "    rna_central_exons['exon_coords'] = rna_central_exons[3].astype('str')+'_'+rna_central_exons[4].astype('str')+','\n",
    "    rna_central_gr_transcripts = rna_central_exons.groupby([0,6,'transcript_id']).agg({'exon_coords':sum}).reset_index()\n",
    "    rna_central_gr_transcripts['transcript_alt_id'] = rna_central_gr_transcripts[0].astype('str')+'_'+rna_central_gr_transcripts[6]+'_'+rna_central_gr_transcripts['exon_coords']\n",
    "\n",
    "    gtf_df_exons = gtf_df.loc[gtf_df[2]=='exon'].reset_index(drop=True)\n",
    "    gtf_df_exons['transcript_id'] = gtf_df_exons[8].str.split('transcript_id \"',expand=True)[1].str.split('\"',expand=True)[0]\n",
    "    gtf_df_exons['exon_coords'] = gtf_df_exons[3].astype('str')+'_'+gtf_df_exons[4].astype('str')+','\n",
    "    gtf_df_gr_transcripts = gtf_df_exons.groupby([0,6,'transcript_id']).agg({'exon_coords':sum}).reset_index()\n",
    "    gtf_df_gr_transcripts['transcript_alt_id'] = gtf_df_gr_transcripts[0].astype('str')+'_'+gtf_df_gr_transcripts[6]+'_'+gtf_df_gr_transcripts['exon_coords']\n",
    "    ensembl_transcripts = list(gtf_df_gr_transcripts['transcript_alt_id'].unique())\n",
    "\n",
    "    preserve_transcripts_list = list(rna_central_gr_transcripts.loc[~rna_central_gr_transcripts['transcript_alt_id'].isin(ensembl_transcripts)]['transcript_id'].unique()) # when a transcript is present in ensemble and RNA central, prioritize ensemble\n",
    "    rna_central_exons = rna_central_exons.loc[rna_central_exons['transcript_id'].isin(preserve_transcripts_list)].reset_index(drop=True)\n",
    "    rna_central['transcript_id'] = rna_central['gene_id']+'.transcript'\n",
    "    rna_central = rna_central.loc[rna_central['transcript_id'].isin(preserve_transcripts_list)].reset_index(drop=True)\n",
    "\n",
    "    rna_central_exons = rna_central_exons[list(range(0,9))+['gene_id','order']]\n",
    "\n",
    "    rna_central_transcripts = rna_central.loc[rna_central[2]=='transcript'].copy().reset_index(drop=True)\n",
    "    rna_central_transcripts[8] = 'gene_id \"'+rna_central_transcripts['gene_id']+'\"; transcript_id \"'+rna_central_transcripts['gene_id']+'.transcript'+'\"; gene_source \"'+rna_central_transcripts['gene_source']+'\"; gene_type \"'+rna_central_transcripts['gene_biotype']+'\"; transcript_source \"'+rna_central_transcripts['gene_source']+'\"; transcript_type \"'+rna_central_transcripts['gene_biotype']+'\"; tag \"'+rna_central_transcripts['gene_source']+'\";'\n",
    "    rna_central_transcripts['order']=2\n",
    "    rna_central_transcripts = rna_central_transcripts[list(range(0,9))+['gene_id','order']]\n",
    "\n",
    "    rna_central_genes = rna_central.loc[rna_central[2]=='transcript'].copy().reset_index(drop=True)\n",
    "    rna_central_genes[2] = 'gene'\n",
    "    rna_central_genes[8] = 'gene_id \"'+rna_central_genes['gene_id']+'\"; gene_source \"'+rna_central_genes['gene_source']+'\"; gene_type \"'+rna_central_genes['gene_biotype']+'\";'\n",
    "    rna_central_genes['order']=1\n",
    "    rna_central_genes = rna_central_genes[list(range(0,9))+['gene_id','order']]\n",
    "\n",
    "    rna_central_gtf = pd.concat([rna_central_genes,rna_central_transcripts,rna_central_exons]).sort_values(['gene_id','order']).reset_index(drop=True).drop(['gene_id','order'],1)\n",
    "    rna_central_gtf[0] = 'chr'+rna_central_gtf[0].astype('str') # so that the naming is consistent\n",
    "    # save standard annotation enriched with RNA central\n",
    "    enriched_gtf = pd.concat([gtf_df,rna_central_gtf]).reset_index(drop=True)\n",
    "    genome_fai = pd.read_csv(file_paths[organism+'_genome_file']+'.fai',delimiter=\"\\t\",index_col=None,header=None)\n",
    "    enriched_gtf = pd.merge(genome_fai[[0]],enriched_gtf,how='inner',on=0)\n",
    "\n",
    "    # remove non-canonical chromosomes, they are not of interest\n",
    "    enriched_gtf = enriched_gtf.loc[enriched_gtf[0].str.startswith('chr')].reset_index(drop=True)\n",
    "    enriched_gtf.to_csv(file_paths[organism+'_enriched_annotation_file'], sep=str('\\t'),header=False,index=None,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8577c-6b26-4d7b-a1ce-c4817c288e34",
   "metadata": {},
   "source": [
    "# Enrich ENCODE metadata using json files from ENCODE portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7df1c42-b5f1-48b5-80f3-21837c78fab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget -i /scicore/home/zavolan/GROUP/RBP_perturbational_networks/metadata/ENCODE_json_urls.txt -P /scicore/home/zavolan/GROUP/RBP_perturbational_networks/metadata/ENCODE_json_files/\n",
      "wget -i /scicore/home/zavolan/GROUP/RBP_perturbational_networks/metadata/ENCODE_fastq_json_urls.txt -P /scicore/home/zavolan/GROUP/RBP_perturbational_networks/metadata/ENCODE_fastq_json_files/\n"
     ]
    }
   ],
   "source": [
    "ENCODE_metadata = pd.read_csv(subdirs['metadata_dir']+'ENCODE_metadata.tsv',delimiter=\"\\t\",\n",
    "                                   index_col=None,header=0)\n",
    "\n",
    "# add information about where are right controls\n",
    "ENCODE_metadata['json_url'] = \"\"\"https://www.encodeproject.org\"\"\"+ENCODE_metadata['File dataset']+\"\"\"?format=json\"\"\"\n",
    "ENCODE_metadata[['json_url']].drop_duplicates().to_csv(subdirs['metadata_dir']+'ENCODE_json_urls.txt', sep=str('\\t'),header=False,index=None,quoting=csv.QUOTE_NONE)\n",
    "\n",
    "command = 'mkdir -p '+subdirs['metadata_dir']+'ENCODE_json_files/'\n",
    "out = subprocess.check_output(command, shell=True)\n",
    "\n",
    "command = \"\"\"wget -i \"\"\"+subdirs['metadata_dir']+'ENCODE_json_urls.txt -P '+subdirs['metadata_dir']+'ENCODE_json_files/'\n",
    "print(command)\n",
    "\n",
    "os.system(\"\"\"find \"\"\"+subdirs['metadata_dir']+'ENCODE_json_files/'+\"\"\" -name '*=json*' > \"\"\"+subdirs['temp_dir']+\"\"\"json_file_paths.tsv\"\"\")\n",
    "json_file_paths = pd.read_csv(subdirs['temp_dir']+'json_file_paths.tsv',delimiter=\"\\t\",\n",
    "                                   index_col=None,header=None)\n",
    "a = []\n",
    "for json_file_path in list(json_file_paths[0]):\n",
    "    json = pd.read_json(json_file_path,orient='index')\n",
    "    if len(json.loc['possible_controls'][0])>0:\n",
    "        a.append([json.loc['accession'][0],json.loc['possible_controls'][0][0]['accession']])\n",
    "    else:\n",
    "        a.append([json.loc['accession'][0],''])\n",
    "controls = pd.DataFrame(a,columns= ['File dataset','File dataset controls'])\n",
    "controls['File dataset'] = '/experiments/'+controls['File dataset']+'/'\n",
    "\n",
    "ENCODE_metadata = pd.merge(ENCODE_metadata,controls,how='left',on=['File dataset'])\n",
    "\n",
    "# add information about which file corresponds to read 1, and which - to read 2\n",
    "\n",
    "ENCODE_metadata['file_json_url'] = \"\"\"https://www.encodeproject.org/files/\"\"\"+ENCODE_metadata['File accession']+\"\"\"/?format=json\"\"\"\n",
    "ENCODE_metadata[['file_json_url']].drop_duplicates().to_csv(subdirs['metadata_dir']+'ENCODE_fastq_json_urls.txt', sep=str('\\t'),header=False,index=None,quoting=csv.QUOTE_NONE)\n",
    "\n",
    "command = 'mkdir -p '+subdirs['metadata_dir']+'ENCODE_fastq_json_files/'\n",
    "out = subprocess.check_output(command, shell=True)\n",
    "\n",
    "command = \"\"\"wget -i \"\"\"+subdirs['metadata_dir']+'ENCODE_fastq_json_urls.txt -P '+subdirs['metadata_dir']+'ENCODE_fastq_json_files/'\n",
    "print(command)\n",
    "\n",
    "os.system(\"\"\"find \"\"\"+subdirs['metadata_dir']+'ENCODE_fastq_json_files/'+\"\"\" -name '*=json*' > \"\"\"+subdirs['temp_dir']+\"\"\"json_fastq_file_paths.tsv\"\"\")\n",
    "json_fastq_file_paths = pd.read_csv(subdirs['temp_dir']+'json_fastq_file_paths.tsv',delimiter=\"\\t\",\n",
    "                                   index_col=None,header=None)\n",
    "\n",
    "a = []\n",
    "for json_file_path in list(json_fastq_file_paths[0]):\n",
    "    json = pd.read_json(json_file_path,orient='index')\n",
    "    a.append([json.loc['accession'][0],json.loc['paired_with'][0],json.loc['paired_end'][0],json.loc['biological_replicates_formatted'][0],json.loc['read_length'][0]])\n",
    "file_metadata = pd.DataFrame(a,columns= ['File accession','paired_with','paired_end','bioreplicate','read_length'])\n",
    "file_metadata['paired_with'] = file_metadata['paired_with'].str.replace('/files/','').str.replace('/','')\n",
    "\n",
    "ENCODE_metadata = pd.merge(ENCODE_metadata,file_metadata,how='left',on=['File accession'])\n",
    "\n",
    "def get_sample(x):\n",
    "    l = [x['File accession'],x['paired_with']]\n",
    "    l.sort()\n",
    "    return '_'.join(l)\n",
    "ENCODE_metadata['sample'] = ENCODE_metadata.apply(lambda x: get_sample(x),1)\n",
    "\n",
    "ENCODE_metadata['File dataset'] = ENCODE_metadata['File dataset'].str.replace('/experiments/','').str.replace('/','')\n",
    "\n",
    "EXPS = ENCODE_metadata.loc[~ENCODE_metadata['File target'].isna()].reset_index(drop=True)\n",
    "EXPS['experiment_id'] = EXPS['File dataset']+'_'+EXPS['File dataset controls']+';'+EXPS['Biosample term name']+';'+EXPS['File target']\n",
    "\n",
    "CONTROLS = ENCODE_metadata.loc[ENCODE_metadata['File target'].isna()].reset_index(drop=True)\n",
    "CONTROLS['File dataset controls'] = CONTROLS['File dataset']\n",
    "tmp = pd.merge(EXPS[['sample','experiment_id','Biosample term name','File target','File dataset controls']],CONTROLS[['File dataset controls','sample']],how='inner',on='File dataset controls')\n",
    "\n",
    "df = pd.melt(tmp,id_vars=['experiment_id','Biosample term name','File target'],value_vars=['sample_x','sample_y'],value_name='sample')[['sample','experiment_id','Biosample term name','File target']].drop_duplicates()\n",
    "\n",
    "# prepare the start samples table in which rows correspond to samples (like in SRA metadata) and experiment ids are provided\n",
    "encode_start_samples = pd.merge(df,ENCODE_metadata.loc[ENCODE_metadata['paired_end']=='1'][['sample','read_length','File target','bioreplicate','Assay term name','File download URL']].rename(columns={'File download URL':'fq1','File target':'EXP_CTL'}),how='left',on='sample')\n",
    "encode_start_samples = pd.merge(encode_start_samples,ENCODE_metadata.loc[ENCODE_metadata['paired_end']=='2'][['sample','File download URL']].rename(columns={'File download URL':'fq2'}),how='left',on='sample')\n",
    "\n",
    "encode_start_samples = encode_start_samples[['sample','fq1','fq2','read_length','bioreplicate','EXP_CTL','experiment_id','Biosample term name','File target','Assay term name']]\n",
    "encode_start_samples['experiment_id'] = encode_start_samples['experiment_id']+';'+(encode_start_samples['Assay term name'].str.contains('shRNA')).astype('str').str.replace('True','KD').replace('False','KO')\n",
    "encode_start_samples['EXP_CTL'] = (encode_start_samples['EXP_CTL'].isna()).astype('str').str.replace('True','CTL').replace('False','EXP')\n",
    "encode_start_samples = encode_start_samples.sort_values(['experiment_id','EXP_CTL']).reset_index(drop=True)\n",
    "encode_start_samples = encode_start_samples.rename(columns = {'File target':'targeted_gene','Biosample term name':'cell_line'})\n",
    "encode_start_samples['assay'] = (encode_start_samples['Assay term name'].str.contains('shRNA')).astype('str').str.replace('True','shRNA').replace('False','CRISPR')\n",
    "\n",
    "encode_start_samples['genome_file'] = file_paths['human_genome_file']\n",
    "encode_start_samples['gtf_file'] = file_paths['human_enriched_annotation_file']\n",
    "encode_start_samples['organism'] = 'human'\n",
    "encode_start_samples = encode_start_samples[['sample','fq1','fq2','read_length','organism','genome_file','gtf_file','experiment_id','targeted_gene','bioreplicate','EXP_CTL','cell_line','assay']]\n",
    "\n",
    "encode_start_samples.to_csv(subdirs['metadata_dir']+'encode_start_samples.tsv', sep=str('\\t'),header=True,index=None,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a48ec430-38c0-4d0b-9fb7-37cc9ef466d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encode_start_samples.drop_duplicates('sample'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9490dd-3e9a-4e3c-bbc7-1f6f69e4a2e7",
   "metadata": {},
   "source": [
    "# Get Tandem Poly(A) Sites for organisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82d8825e-4e7c-4a34-913e-57a34f803200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat /scicore/home/zavolan/GROUP/Genomes/homo_sapiens/hg38_v42/atlas.clusters.2.0.GRCh38.96.bed.gz | sed -E 's/^([0-9]+|[XYM])/chr\\1/' | pigz --stdout > /scicore/home/zavolan/GROUP/Genomes/homo_sapiens/hg38_v42/atlas.clusters.2.0.GRCh38.96.wchr.bed.gz && mv /scicore/home/zavolan/GROUP/Genomes/homo_sapiens/hg38_v42/atlas.clusters.2.0.GRCh38.96.wchr.bed.gz /scicore/home/zavolan/GROUP/Genomes/homo_sapiens/hg38_v42/atlas.clusters.2.0.GRCh38.96.bed.gz\n"
     ]
    }
   ],
   "source": [
    "# add 'chr' to the polyAsite Atlas file\n",
    "\n",
    "command = \"\"\"zcat \"\"\"+file_paths['human_polyAsite_atlas']+r\"\"\" | sed -E 's/^([0-9]+|[XYM])/chr\\1/' | pigz --stdout > \"\"\"+file_paths['human_polyAsite_atlas'].replace('.bed.gz','.wchr.bed.gz')+\\\n",
    "\"\"\" && mv \"\"\"+file_paths['human_polyAsite_atlas'].replace('.bed.gz','.wchr.bed.gz')+' '+file_paths['human_polyAsite_atlas']\n",
    "print(command)\n",
    "# run the command in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81eb2829-ab86-478a-bfef-c6ddfe967635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify .yaml file for the tandem-pas pipeline\n",
    "\n",
    "yaml = ruamel.yaml.YAML()\n",
    "yaml.preserve_quotes = True\n",
    "with open(subdirs['tandem_PAS_dir']+'configs/config.yaml') as f_read:\n",
    "    data = yaml.load(f_read)\n",
    "\n",
    "data['polyasites'] = file_paths['human_polyAsite_atlas']\n",
    "data['atlas_version'] = 'v2.0'\n",
    "data['gtf'] = file_paths['human_enriched_annotation_file']\n",
    "\n",
    "with open(subdirs['tandem_PAS_dir']+'configs/config_hg38.yaml','w') as f_write:     \n",
    "    yaml.dump(data, f_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29cad7b5-b717-4827-a9f7-5eea5c177eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scicore/home/zavolan/mirono0000/libs/tandem-pas/'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs['tandem_PAS_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e882fb07-b205-4369-b85a-9e7572f6d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snakemake --snakefile Snakefile --configfile configs/config_hg38.yaml --printshellcmds --use-singularity --singularity-args \"--bind ${PWD}/../,/scicore/home/zavolan/GROUP/Genomes/homo_sapiens/\" --cluster-config configs/cluster_config.json --cores 500 --local-cores 10 --cluster \"sbatch --cpus-per-task {cluster.threads} --mem {cluster.mem} --qos {cluster.queue} --time {cluster.time} -o {params.cluster_log} --export=JOB_NAME={rule} --open-mode=append\" --jobs 20 --nolock -np'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run tandem-pas WF with activated env\n",
    "\n",
    "command = \"\"\"snakemake \\\n",
    "--snakefile Snakefile \\\n",
    "--configfile configs/config_hg38.yaml \\\n",
    "--printshellcmds \\\n",
    "--use-singularity \\\n",
    "--singularity-args \"--bind ${PWD}/../,\"\"\"+subdirs['human_annotation_dir']+\"\"\"\" \\\n",
    "--cluster-config configs/cluster_config.json \\\n",
    "--cores 500 \\\n",
    "--local-cores 10 \\\n",
    "--cluster \"sbatch \\\n",
    "--cpus-per-task {cluster.threads} \\\n",
    "--mem {cluster.mem} \\\n",
    "--qos {cluster.queue} \\\n",
    "--time {cluster.time} \\\n",
    "-o {params.cluster_log} \\\n",
    "--export=JOB_NAME={rule} \\\n",
    "--open-mode=append\" \\\n",
    "--jobs 20 \\\n",
    "--nolock \\\n",
    "-np\"\"\"\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60c347-2619-4648-adc9-e46c36561dd7",
   "metadata": {},
   "source": [
    "# Prepare .yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8f136e6b-634c-4da4-a6b5-64df594fb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load default rule_config, modify it and save\n",
    "\n",
    "WF_version = 'v1'\n",
    "\n",
    "yaml = ruamel.yaml.YAML()\n",
    "yaml.preserve_quotes = True\n",
    "with open(subdirs['wf_dir']+'config.yaml') as f_read:\n",
    "    data = yaml.load(f_read)\n",
    "data['samples_file'] = subdirs['metadata_dir']+'encode_start_samples.tsv'\n",
    "\n",
    "data['output_dir'] = subdirs['wf_runs_dir']+WF_version+'/output/'\n",
    "data['local_log'] = subdirs['wf_runs_dir']+WF_version+'/output/local_log/'\n",
    "data['cluster_log'] = subdirs['wf_runs_dir']+WF_version+'/output/cluster_log/'\n",
    "\n",
    "command = 'mkdir -p '+subdirs['wf_runs_dir']+WF_version+'/output/'\n",
    "out = subprocess.check_output(command, shell=True)\n",
    "\n",
    "with open(subdirs['wf_runs_dir']+WF_version+'/config.yaml','w') as f_write:     \n",
    "    yaml.dump(data, f_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9c64876a-71f7-4414-a7e0-19c1888ea0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scicore/home/zavolan/GROUP/RBP_perturbational_networks/wf_runs/v1/output/cluster_log/'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs['wf_runs_dir']+WF_version+'/output/cluster_log/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503e062-3f22-47f5-826b-adc0f0f062fe",
   "metadata": {},
   "source": [
    "# Run main WF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21562599-9b2d-488d-812d-637bc7438dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scicore/home/zavolan/mirono0000/Projects/RBP_perturbational_networks/WF/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs['wf_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6baeae43-e94c-4dee-a5c2-57c645d19787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snakemake --snakefile Snakefile --configfile /scicore/home/zavolan/GROUP/RBP_perturbational_networks/wf_runs/v1/config.yaml --printshellcmds --use-conda --conda-frontend conda --use-singularity --singularity-args \"--bind /scicore/home/zavolan/GROUP/RBP_perturbational_networks/,/scicore/home/zavolan/GROUP/Genomes/homo_sapiens/\" --cluster-config cluster.json --cores 500 --local-cores 10 --jobs 100 --latency-wait 60 --cluster \"sbatch --cpus-per-task={cluster.threads} --mem={cluster.mem} --qos={cluster.queue} --partition={cluster.partition} --time={cluster.time} --output={cluster.out}\" --nolock -np'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WF_version = 'v1'\n",
    "\n",
    "command = \"\"\"snakemake \\\n",
    "--snakefile Snakefile \\\n",
    "--configfile \"\"\"+subdirs['wf_runs_dir']+WF_version+'/config.yaml'+\"\"\" \\\n",
    "--printshellcmds \\\n",
    "--use-conda --conda-frontend conda \\\n",
    "--use-singularity \\\n",
    "--singularity-args \"--bind \"\"\"+subdirs['main_project_dir']+','+subdirs['human_annotation_dir']+\"\"\"\" \\\n",
    "--cluster-config cluster.json \\\n",
    "--cores 500 \\\n",
    "--local-cores 10 \\\n",
    "--jobs 100 \\\n",
    "--latency-wait 60 \\\n",
    "--cluster \"sbatch \\\n",
    "--cpus-per-task={cluster.threads} \\\n",
    "--mem={cluster.mem} \\\n",
    "--qos={cluster.queue} \\\n",
    "--partition={cluster.partition} \\\n",
    "--time={cluster.time} \\\n",
    "--output={cluster.out}\" \\\n",
    "--nolock \\\n",
    "-np\"\"\"\n",
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db777d2-bcfa-49eb-8996-21419ca1a6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de40031-fb20-40c9-b61e-7662e850095c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695914f0-7f0b-4c92-8879-0ea00a7f36f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672da22d-9f37-4a96-99e6-5cc4e6623a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215b726-de9e-4575-9c0b-59a6ce44320f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953417ff-baf7-494e-bd5c-41fcd1466f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57631bbb-e0de-45f3-a225-5d6ccf9ce3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-py310]",
   "language": "python",
   "name": "conda-env-miniconda3-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
